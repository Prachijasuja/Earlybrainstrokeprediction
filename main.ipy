import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.impute import SimpleImputer
from imblearn.over_sampling import SMOTE
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
import tensorflow as tf
from tensorflow import keras
from keras import Sequential
from keras.layers import Dense, Input

import joblib

# Load the dataset
stroke_data = pd.read_csv("healthcare-dataset-stroke-data.csv")

# Data Exploration and Preprocessing
print("Unique values in 'stroke' column:", stroke_data.stroke.unique())
print("\nValue counts for 'stroke' column:\n", stroke_data.stroke.value_counts())
print("\nPercentage distribution for 'stroke' column:\n", stroke_data.stroke.value_counts(True).rename('%').mul(100))
print("\nSample of 'stroke' column (20):\n", stroke_data["stroke"].sample(20))
print("\nLast 3 rows of the dataframe:\n", stroke_data.tail(3))
print("\nDescriptive statistics:\n", stroke_data.describe())
sns.countplot(data=stroke_data, x='gender')
plt.title('Number of patients by gender')
plt.show()

print("\nSample of the dataframe (20):\n", stroke_data.sample(20))
print("\nMissing values per column:\n", stroke_data.isnull().sum())

cat_features = [feature for feature in stroke_data.columns if stroke_data[feature].dtype == 'O']
print("\nNumber of categorical features:", len(cat_features))
print("*" * 80)
print('Categorical variables column name:', cat_features)

numerical_features = [feature for feature in stroke_data.columns if stroke_data[feature].dtype != 'O']
print("*" * 80)
print("Numerical variables Columns", numerical_features)
print("\nFirst 5 rows of categorical features (in a DataFrame):\n", pd.DataFrame(cat_features).head())
print("\nFirst 5 rows of the dataframe:\n", stroke_data.head())
print("\nCategorical features list:", cat_features)

print("\nNumber of duplicated rows:", stroke_data.duplicated().sum())
print("\nUnique values in 'Residence_type':", stroke_data['Residence_type'].unique())
print("Number of unique values in 'avg_glucose_level':", stroke_data['avg_glucose_level'].nunique())
print("Unique values in 'work_type':", stroke_data['work_type'].unique())
print("Number of unique values in 'bmi':", stroke_data['bmi'].nunique())

stroke_data2 = stroke_data.select_dtypes(include=['number'])
corr = stroke_data2.corr()
plt.figure(figsize=(12, 10))
sns.heatmap(data=corr, annot=True, cmap='Spectral').set(title="Correlation Matrix")
plt.show()
corr_matrix = stroke_data2.corr().round(2)
print("\nCorrelation Matrix:\n", corr_matrix)

mask = np.triu(np.ones_like(corr_matrix, dtype=bool))
plt.figure(figsize=(12, 10))
sns.heatmap(corr_matrix, mask=mask, annot=True).set(title="Correlation Matrix (upper triangle)")
plt.show()

for col in cat_features[:]:
    plt.figure(figsize=(6, 3), dpi=100)
    sns.countplot(data=stroke_data, x=col, hue='stroke', palette='gist_rainbow_r')
    plt.legend(loc=(1.05, 0.5))
    plt.title(f'Stroke distribution by {col}')
    plt.show()

# Feature Selection: Only include the 10 features for scaling and modeling
features_to_include = [
    'gender', 'age', 'hypertension', 'heart_disease', 'ever_married', 
    'work_type', 'Residence_type', 'avg_glucose_level', 'smoking_status', 'bmi'
]

# Update dataset to include only the selected features plus 'stroke' as the target
stroke_data = stroke_data[features_to_include + ['stroke']]

# Encode categorical variables
stroke_data['gender'] = stroke_data['gender'].map({'Male': 0, 'Female': 1})
stroke_data['ever_married'] = stroke_data['ever_married'].map({'No': 0, 'Yes': 1})
stroke_data['work_type'] = stroke_data['work_type'].map({'Private': 0, 'Self-employed': 1, 'Govt_job': 2, 'children': 3, 'Never_worked': 4})
stroke_data['Residence_type'] = stroke_data['Residence_type'].map({'Rural': 0, 'Urban': 1})
stroke_data['smoking_status'] = stroke_data['smoking_status'].map({'never smoked': 0, 'smokes': 1, 'formerly smoked': 2, 'Unknown': 3})

# Handle missing values (in 'bmi' column)
stroke_data['bmi'].fillna(stroke_data['bmi'].mean(), inplace=True)

# Split dataset into features and target
X = stroke_data.drop(columns=['stroke'])
y = stroke_data['stroke']

# Impute any missing values if present
imputer = SimpleImputer(strategy='mean')
X = imputer.fit_transform(X)
y = imputer.fit_transform(y.values.reshape(-1, 1))

# Apply StandardScaler only on the selected 10 features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)  # This scales only the 10 features you selected

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=7)
print("\nShape of X_train:", X_train.shape, "Shape of X_test:", X_test.shape)

# Handle class imbalance using SMOTE
smote = SMOTE(random_state=3)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

# Logistic Regression Model
lr = LogisticRegression(max_iter=200)
lr.fit(X_train_smote, y_train_smote.ravel())  # Use .ravel() for y_train
y_pred1 = lr.predict(X_test)
accuracy_lr = accuracy_score(y_test, y_pred1)
print("\nAccuracy Score (Logistic Regression):", accuracy_lr)

# Confusion Matrix for Logistic Regression
cn_lr = confusion_matrix(y_test, y_pred1)
conf_matrix_lr = pd.DataFrame(data=cn_lr, columns=['Predicted:0', 'Predicted:1'], index=['Actual:0', 'Actual:1'])
plt.figure(figsize=(8, 5))
sns.heatmap(conf_matrix_lr, annot=True, fmt='d', cmap="YlGnBu")
plt.title('Confusion Matrix (Logistic Regression)')
plt.show()
print("\nClassification Report (Logistic Regression):\n", classification_report(y_test, y_pred1))

# K-Nearest Neighbors (KNN) Model
knn_model = KNeighborsClassifier(n_neighbors=5)
knn_model.fit(X_train_smote, y_train_smote.ravel())  # Use .ravel() for y_train
y_pred_knn = knn_model.predict(X_test)

test_knn = pd.DataFrame({'Actual': y_test.ravel(), 'Predicted': y_pred_knn})
print("\nFirst 5 rows of Actual vs Predicted (KNN):\n", test_knn.head())

accuracy_knn = accuracy_score(y_test, y_pred_knn)
print("\nAccuracy Score (KNN):", accuracy_knn)

# Neural Network Model (TensorFlow/Keras)
nn_model = Sequential([
    Input(shape=(X_train_smote.shape[1],)),  # Use Input() layer for input shape
    Dense(4800, activation='relu'),
    Dense(2000, activation='relu'),
    Dense(1000, activation='relu'),
    Dense(1, activation='sigmoid')
])

nn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train model with reduced epochs for debugging
history = nn_model.fit(X_train_smote, y_train_smote, epochs=5, batch_size=32, validation_split=0.2, verbose=1)  # Set verbose to 1

# Predict with NN
y_pred_nn_prob = nn_model.predict(X_test)
y_pred_nn_class = (y_pred_nn_prob > 0.5).astype(int)

# Calculate accuracy for NN
accuracy_nn = accuracy_score(y_test, y_pred_nn_class)
print("\nAccuracy Score (Neural Network):", accuracy_nn)

# Confusion matrix for Neural Network
cn_nn = confusion_matrix(y_test, y_pred_nn_class)
conf_matrix_nn = pd.DataFrame(data=cn_nn, columns=['Predicted: 0', 'Predicted: 1'], index=['Actual: 0', 'Actual: 1'])
plt.figure(figsize=(8, 5))
sns.heatmap(conf_matrix_nn, annot=True, fmt='d', cmap="YlGnBu")
plt.title('Confusion Matrix (Neural Network)')
plt.show()

print("\nClassification Report (Neural Network):\n", classification_report(y_test, y_pred_nn_class))

# Model Accuracies Comparison
accuracies = {
    'Logistic Regression': accuracy_lr,
    'K-Nearest Neighbors': accuracy_knn,
    'Neural Network': accuracy_nn
}

print("\nModel Accuracies Comparison:")
for model, acc in accuracies.items():
    print(f"{model}: {acc:.4f}")

# Save the KNN model, or you can replace it with another model like Logistic Regression or NN
joblib.dump(knn_model, 'stroke_prediction_model.pkl')

# Save the scaler (important for preprocessing)
joblib.dump(scaler, 'scaler.pkl')
